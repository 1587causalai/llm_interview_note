# 企业A

# 一面

### Transformer部分

1.  Transformer整体介绍
2.  Self-attention 的机制和原理
3.  self-attention为什么用qkv，使用qv可以不？
4.  计算A→B的注意力和B→A的注意力的区别，如果使用qv能不能区分这两个

### 微调部分

1.  为什么需要微调？如果需要专业的数据，外挂数据库就可以解决的。
2.  数据集怎么获取？
3.  介绍LoRA微调，及其微调中的一些重要参数
4.  微调中碰到那些问题？
5.  微调的硬件设备是怎么样的？
6.  如果显存不够，怎么解决？
7.  微调的Loss是怎么变化的？
8.  微调完成后，怎么测试实际效果？
9.  除了LoRA，还用过其他微调的方法吗？

### 分布式训练

1.  介绍各个并行，数据并行、模型并行
2.  介绍MoE，MoE怎么使用到大模型上
3.  MoE并行

### 训练部分

1.  训练时间怎么计算？
2.  参数量怎么计算？
3.  英文到中文的词表映射怎么做？
4.  DPO算法
5.  介绍PPO算法

### 场景题

**背景**：假设做一个智能客服（提问有顺序），消费者提出问题，智能客服回答；整个流程有一定的顺序。有三个外挂数据库，一个负责业务流程，一个回答专业问题，剩下一个忘记了。将消费者的提问，结合这三个数据库中的数据，组成prompt，送到大模型中生成答案。

Q：一个问题经过多个数据库，prompt太长了，怎么解决这个问题？

A：1：压缩prompt长度，对比多个不同的prompt，选择与问题相关的prompt，尽可能短；2：消费者提的问题，可以使用实体命名识别等技术，抽取关键字构造prompt，而不是全部构造prompt

Q：这个流程直接送给大模型效果不太好，应该怎么处理：

A：分阶段，分步骤处理。一个模型处理一部分问题，而不是把整个任务流程丢给大模型处理。

# 二面

### 技术问题

1.  介绍项目中的LoRA微调？
2.  微调的时候，出现了什么问题？
3.  还有了解其他微调技术吗？详细讲述一下。
    -   具体了解有四大类大模型微调技术：
    1.  增加额外参数：Prefix tuning, prompt tuning
    2.  指定更新一部分参数：BitFit
    3.  重参数化微调：LoRA，AdaptLoRA，QLoRA
    4.  混合高效微调：UniPELT
4.  了解RAG技术吗？详细讲述一下

### LLM宏观问题

1.  你是什么时候关注大模型的？
2.  了解国内的大模型有哪些吗？
    1.  ChatGLM, 文心，讯飞大模型 。。。。
3.  你对大模型未来的方向怎么看？
    1.  底层研究方面：
        1.  Nvidia算力增长，对Transformer进一步优化，媲美CNN的速度；
        2.  大模型架构会以Transformer decoder为主题，研究较多的变种。
    2.  大模型研究方面：
        1.  参数进一步增加，性能进一步提升。
        2.  通过模型持续学习、增加记忆机制、突破这三元组知识表示方法等进一步提升大模型认知能力。
        3.  在模型本身方面，多模态、多语言、面向垂直领域的新模型也会成为研究重点。
    3.  大模型应用方面：使用大模型门槛会大大降低，促使形成“大模型+少量数据微调”的AI工业化开发模式：
        1.  降成本，提速度：推理，模型剪枝，模型压缩
        2.  搭平台：大公司会提供一站式大模型开发应用平台，提供模型在线构建、微调、部署、发布的全流程服务，能够支持成百上千个应用的开发和部署。

### 新了解到的知识

1.  0-1 LLM和 deepseek全量微调在公司的垂直领域业务上效果最好，其次是ChatGLM4和文心4不开源的接口。
2.  在大模型部署方面，还是vLLM效果最好；而 Nvidia 的TensorRT-LLM效果不太行，容易出现很多问题
