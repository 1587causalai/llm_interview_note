# 21.Attention升级

-   1 传统 Attention 存在哪些问题？
-   2 Attention 优化方向
-   3 Attention 变体有哪些？
-   4 Multi-Query Attention 篇
    -   4.1 Multi-head Attention 存在什么问题？
    -   4.2 介绍一下 Multi-Query Attention？
    -   4.3 对比一下 Multi-head Attention 和 Multi-Query Attention？
    -   4.4 Multi-Query Attention 这样做的好处是什么？
    -   4.5 有 哪些模型 是 使用 Multi-Query Attention？
-   5 Grouped-query Attention
    -   5.1 什么是 Grouped-query Attention？
    -   5.2 有哪些大模型使用 Grouped-query Attention？
-   6 FlashAttention 介绍一下
-   7 并行 transformer block 介绍一下？
