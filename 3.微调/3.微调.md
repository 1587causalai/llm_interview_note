# 3.微调

1.  如果想要在某个模型基础上做全参数微调，究竟需要多少显存？
2.  为什么SFT之后感觉LLM傻了?
3.  SFT 指令微调数据 如何构建?
4.  领域模型Continue PreTrain 数据选取？
5.  领域数据训练后，通用能力往往会有所下降，如何缓解模型遗忘通用能力？
6.  领域模型Continue PreTrain ，如何 让模型在预训练过程中就学习到更多的知识？
7.  进行SFT操作的时候，基座模型选用Chat还是Base?
8.  领域模型微调 指令&数据输入格式 要求？
9.  领域模型微调 领域评测集 构建？
10. 领域模型词表扩增是不是有必要的？
11. 如何训练自己的大模型？
12. 训练中文大模型有啥经验？
13. 指令微调的好处？
14. 预训练和微调哪个阶段注入知识的？
15. 想让模型学习某个领域或行业的知识，是应该预训练还是应该微调？
16. 多轮对话任务如何微调模型？
17. 微调后的模型出现能力劣化，灾难性遗忘是怎么回事？
18. 微调模型需要多大显存？
19. 大模型LLM进行SFT操作的时候在学习什么？
20. 预训练和SFT操作有什么不同
21. 样本量规模增大，训练出现OOM错
22. 大模型LLM进行SFT 如何对样本进行优化？
23. 模型参数迭代实验
24. 微调大模型的一些建议
