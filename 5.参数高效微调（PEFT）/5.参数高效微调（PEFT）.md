# 5.参数高效微调（PEFT）

## 5.1 基本概念

-   微调方法是啥？如何微调？
-   为什么需要 PEFT？
-   介绍一下 PEFT？
-   PEFT 有什么优点？
-   微调方法批处理大小模式GPU显存速度？
-   Peft 和 全量微调区别？
-   多种不同的高效微调方法对比
-   当前高效微调技术存在的一些问题
-   高效微调技术最佳实践
-   PEFT 存在问题？
-   能不能总结一下各种参数高效微调方法？

## 5.2 Aadapter-tuning

-   一、为什么 需要 适配器微调（Adapter-tuning）？
-   二、适配器微调（Adapter-tuning）思路？
-   三、 适配器微调（Adapter-tuning）特点是什么？
-   四、AdapterFusion 思路 是什么？
-   五、AdapterDrop 思路 是什么？
-   六、AdapterDrop 特点 是什么？
-   七、MAM Adapter 思路 是什么？
-   八、MAM Adapter 特点 是什么？

## 5.3 Prompting

-   一、为什么需要 提示学习（Prompting）？
-   二、什么是 提示学习（Prompting）？
-   三、提示学习（Prompting） 有什么优点？
-   四、提示学习（Prompting）有哪些方法，能不能稍微介绍一下它们间？
    -   4.1 前缀微调（Prefix-tining）篇
        -   4.1.1 为什么需要 前缀微调（Prefix-tining）？
        -   4.1.2 前缀微调（Prefix-tining）思路是什么？
        -   4.1.3 前缀微调（Prefix-tining）的优点是什么？
        -   4.1.4 前缀微调（Prefix-tining）的缺点是什么？
    -   4.2 指示微调（Prompt-tuning）篇
        -   4.2.1 为什么需要 指示微调（Prompt-tuning）？
        -   4.2.2 指示微调（Prompt-tuning）思路是什么？
        -   4.2.3 指示微调（Prompt-tuning）优点是什么？
        -   4.2.4 指示微调（Prompt-tuning）缺点是什么？
        -   4.2.5 指示微调（Prompt-tuning）与 Prefix-tuning 区别 是什么？
        -   4.2.6 指示微调（Prompt-tuning）与 fine-tuning 区别 是什么？
    -   4.3 P-tuning 篇
        -   4.3.1 为什么需要 P-tuning？
        -   4.3.2 P-tuning 思路是什么？
        -   4.3.3 P-tuning 优点是什么？
        -   4.3.4 P-tuning 缺点是什么？
    -   4.4 P-tuning v2 篇
        -   4.4.1 为什么需要 P-tuning v2？
        -   4.4.2 P-tuning v2 思路是什么？
        -   4.4.3 P-tuning v2 优点是什么？
        -   4.4.4 P-tuning v2 缺点是什么？

## 5.4 LoRA

-   一、LoRA篇
    -   1.1 什么是 LoRA？
    -   1.2 LoRA 的思路是什么？
    -   1.3 LoRA 的特点是什么？
-   二、QLoRA篇
    -   2.1 QLoRA 的思路是怎么样的？
    -   2.2 QLoRA 的特点是什么？
-   三、AdaLoRA篇
    -   3.1 AdaLoRA 的思路是怎么样的？
-   四、LoRA权重是否可以合入原模型？
-   五、ChatGLM-6B LoRA后的权重多大？
-   六、LoRA 微调优点是什么？
-   七、LoRA微调方法为啥能加速训练？
-   八、如何在已有LoRA模型上继续训练？
-   九、LoRA 缺点是什么？
-   十、LoRA这种微调方法和全参数比起来有什么劣势吗？
