# 6.推理

-   1\.  为什么大模型推理时显存涨的那么多还一直占着？
-   2\.  大模型在gpu和cpu上推理速度如何？
-   3\.  推理速度上，int8和fp16比起来怎么样？
-   4\.  大模型有推理能力吗？
-   5\.  大模型生成时的参数怎么设置？
-   6\.  有哪些省内存的大语言模型训练/微调/推理方法？
    -   6.1 如何 估算模型所需的RAM？
    -   6.2 Fp16-mixed precision
    -   6.3 Int8-bitsandbytes
    -   6.4 LoRA
    -   6.5 Gradient Checkpointing
    -   6.6 Torch FSDP+CPU offload
-   7\.  如何让大模型输出合规化
-   8\.  应用模式变更
