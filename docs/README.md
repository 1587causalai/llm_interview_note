# LLM Knowledge and Interview Questions

## Table of Contents

* [Home](/)
* [Real Interview Questions](/ch1)
* [01.LLM Fundamentals](/01.llm_fundamentals/)
  * [1.1 LLM Development History](/01.llm_fundamentals/)
    * [1.Language Models](/01.llm_fundamentals/1.language_models/1.language_models.md "Language Models")
  * [1.2 Word Segmentation and Word Vectors](/01.llm_fundamentals)
    * [1.Word Segmentation](/01.llm_fundamentals/1.tokenization/1.tokenization.md)
    * [2.Jieba Tokenization Usage and Principles](/01.llm_fundamentals/2.jieba_tokenization/2.jieba_tokenization.md)
    * [3.Part of Speech Tagging](/01.llm_fundamentals/3.pos_tagging/3.pos_tagging.md)
    * [4.Syntactic Analysis](/01.llm_fundamentals/4.syntax_analysis/4.syntax_analysis.md "Syntactic Analysis")
    * [5.Word Vectors](/01.llm_fundamentals/5.word_vectors/5.word_vectors.md "Word Vectors")
  * [1.3 Language Model Fundamentals](/01.llm_fundamentals/)
    * [Word2Vec](/01.llm_fundamentals/word2vec/word2vec.md "Word2Vec")
    * [NLP Feature Extractors (CNN/RNN/TF)](/01.llm_fundamentals/feature_extractors_cnn_rnn_tf/feature_extractors.md)
    * [NLP Interview Questions](/01.llm_fundamentals/nlp_interview_questions/nlp_interview_questions.md "NLP Interview Questions")
    * [Why LLM Uses Decoder-only Architecture](/01.llm_fundamentals/decoder_only_architecture/decoder_only_architecture.md "Why LLM Uses Decoder-only Architecture")
  * [1.4 Deep Learning](/01.llm_fundamentals/)
    * [1.Activation Functions](/01.llm_fundamentals/1.activation_functions/1.activation_functions.md)
  * [1.5 Questions](/01.llm_fundamentals/)
    * [1.LLM Concepts](/01.llm_fundamentals/1.llm_concepts/1.llm_concepts.md)
* [02.LLM Architecture](/02.llm_architecture/)
  * [2.1 Transformer Model](/02.llm_architecture/)
    * [1.attention](/02.llm_architecture/1.attention/1.attention.md "1.attention")
    * [2.layer\_normalization](/02.llm_architecture/2.layer_normalization/2.layer_normalization.md "2.layer_normalization")
    * [3.position encoding](/02.llm_architecture/3.position_encoding/3.position_encoding.md "3.position encoding")
    * [4.tokenization](/02.llm_architecture/4.tokenization/4.tokenization.md "4.tokenization")
    * [5.token and model parameters](/02.llm_architecture/5.token_and_model_parameters/5.token_and_model_parameters.md "5.token and model parameters")
    * [6.activation functions](/02.llm_architecture/6.activation_functions/6.activation_functions.md "6.activation functions")
  * [2.2 Attention](/02.llm_architecture/)
    * [MHA\_MQA\_GQA](/02.llm_architecture/MHA_MQA_GQA/MHA_MQA_GQA.md "MHA_MQA_GQA")
  * [2.3 Decoding](/02.llm_architecture/)
    * [Decoding Strategies (Top-k & Top-p & Temperature)](/02.llm_architecture/decoding_strategies/decoding_strategies.md "Decoding Strategies (Top-k & Top-p & Temperature)")
  * [2.4 BERT](/02.llm_architecture/)
    * [bert details](/02.llm_architecture/bert_details/bert_details.md "bert details")
    * [Transformer Architecture Details](/02.llm_architecture/transformer_architecture_details/transformer_architecture_details.md "Transformer Architecture Details")
    * [bert variants](/02.llm_architecture/bert_variants/bert_variants.md "bert variants")
  * [2.5 Common LLM Models](/02.llm_architecture/)
    * [llama series models](/02.llm_architecture/llama_series_models/llama_series_models.md "llama series models")
    * [chatglm series models](/02.llm_architecture/chatglm_series_models/chatglm_series_models.md "chatglm series models")
    * [llama 2 code explanation](/02.llm_architecture/llama_2_code_explanation/llama_2_code_explanation.md "llama 2 code explanation")
    * [llama 3](/02.llm_architecture/llama_3/llama_3.md "llama 3")
  * [2.6 MoE](/02.llm_architecture/)
    * [1.MoE Paper](/02.llm_architecture/1.MoE_paper/1.MoE_paper.md "1.MoE Paper")
    * [2.Classic MoE Papers](/02.llm_architecture/2.classic_MoE_papers/2.classic_MoE_papers.md "2.Classic MoE Papers")
    * [3.LLM MoE: Switch Transformers](/02.llm_architecture/3.LLM_MoE_Switch_Transformers/3.LLM_MoE_Switch_Transformers.md "3.LLM MoE: Switch Transformers")
* [03.Training Datasets](/03.training_datasets/)
  * [3.1 Datasets](/03.training_datasets/)
    * [Data Formats](/03.training_datasets/data_formats/data_formats.md "Data Formats")
  * [3.2 Model Parameters](/03.training_datasets/)
* [04.Distributed Training](/04.distributed_training/)
  * [4.1 Basic Knowledge](/04.distributed_training/)
    * [1.Overview](/04.distributed_training/1.overview/1.overview.md "1.Overview")
    * [2.Data Parallelism](/04.distributed_training/2.data_parallelism/2.data_parallelism.md "2.Data Parallelism")
    * [3.Pipeline Parallelism](/04.distributed_training/3.pipeline_parallelism/3.pipeline_parallelism.md "3.Pipeline Parallelism")
    * [4.Tensor Parallelism](/04.distributed_training/4.tensor_parallelism/4.tensor_parallelism.md "4.Tensor Parallelism")
    * [5.Sequence Parallelism](/04.distributed_training/5.sequence_parallelism/5.sequence_parallelism.md "5.Sequence Parallelism")
    * [6.Multi-Dimensional Mixed Parallelism](/04.distributed_training/6.multi_dimensional_mixed_parallelism/6.multi_dimensional_mixed_parallelism.md "6.Multi-Dimensional Mixed Parallelism")
    * [7.Automatic Parallelism](/04.distributed_training/7.automatic_parallelism/7.automatic_parallelism.md "7.Automatic Parallelism")
    * [8.moe parallelism](/04.distributed_training/8.moe_parallelism/8.moe_parallelism.md "8.moe parallelism")
    * [9.Summary](/04.distributed_training/9.summary/9.summary.md "9.Summary")
  * [4.2 DeepSpeed](/04.distributed_training/)
    * [DeepSpeed Introduction](/04.distributed_training/deepspeed_introduction/deepspeed_introduction.md "DeepSpeed Introduction")
  * [4.3 Megatron](/04.distributed_training/)
  * [4.4 Training Acceleration](/04.distributed_training/)
  * [4.5 Useful Articles](/04.distributed_training/)
  * [4.6 Questions](/04.distributed_training/)
    * [1.Distributed Training Questions](/04.distributed_training/distributed_training_questions/distributed_training_questions.md "Distributed Training Questions")
    * [2.Memory Issues](/04.distributed_training/1.memory_issues/1.memory_issues.md "2.Memory Issues")
* [05.Supervised Finetuning](/05.supervised_finetuning/)
  * [5.1 Theory](/05.supervised_finetuning/)
    * [1.Basic Concepts](/05.supervised_finetuning/1.basic_concepts/1.basic_concepts.md "1.Basic Concepts")
    * [2.Prompting](/05.supervised_finetuning/2.prompting/2.prompting.md "2.Prompting")
    * [3.adapter-tuning](/05.supervised_finetuning/3.adapter_tuning/3.adapter_tuning.md "3.adapter-tuning")
    * [4.lora](/05.supervised_finetuning/4.lora/4.lora.md "4.lora")
    * [5.Summary](/05.supervised_finetuning/5.summary/5.summary.md "5.Summary")
  * [5.2 Fine-tuning Practice](/05.supervised_finetuning/)
    * [llama2 fine-tuning](/05.supervised_finetuning/llama2_fine_tuning/llama2_fine_tuning.md "llama2 fine-tuning")
    * [ChatGLM3 fine-tuning](/05.supervised_finetuning/ChatGLM3_fine_tuning/ChatGLM3_fine_tuning.md "ChatGLM3 fine-tuning")
  * [5.3 Questions](/05.supervised_finetuning/)
    * [1.Fine-tuning](/05.supervised_finetuning/1.fine_tuning/1.fine_tuning.md "1.Fine-tuning")
    * [2.Pre-training](/05.supervised_finetuning/2.pre_training/2.pre_training.md "2.Pre-training")
* [06.Inference](/06.inference/)
  * [6.1 Inference Frameworks](/06.inference/)
    * [0.LLM Inference Framework Summary](/06.inference/0.llm_inference_framework_summary/0.llm_inference_framework_summary.md "0.LLM Inference Framework Summary")
    * [1.vllm](/06.inference/1.vllm/1.vllm.md "1.vllm")
    * [2.text_generation\_inference](/06.inference/2.text_generation_inference/2.text_generation_inference.md "2.text_generation_inference")
    * [3.faster_transformer](/06.inference/3.faster_transformer/3.faster_transformer.md "3.faster_transformer")
    * [4.trt_llm](/06.inference/4.trt_llm/4.trt_llm.md "4.trt_llm")
  * [6.2 Inference Optimization Techniques](/06.inference/)
    * [LLM Inference Optimization Techniques](/06.inference/llm_inference_optimization_techniques/llm_inference_optimization_techniques.md "LLM Inference Optimization Techniques")
  * [6.3 Quantization](/06.inference/)
  * [6.4 vLLM](/06.inference/)
  * [6.5 Questions](/06.inference/)
    * [1.Inference](/06.inference/1.inference/1.inference.md "1.Inference")
* [07.Reinforcement Learning](/07.reinforcement_learning/)
  * [7.1 Reinforcement Learning Principles](/07.reinforcement_learning/)
    * [Policy Gradient (pg)](/07.reinforcement_learning/policy_gradient_pg/policy_gradient_pg.md "Policy Gradient (pg)")
    * [Proximal Policy Optimization (ppo)](/07.reinforcement_learning/proximal_policy_optimization_ppo/proximal_policy_optimization_ppo.md "Proximal Policy Optimization (ppo)")
  * [7.2 RLHF](/07.reinforcement_learning/)
    * [Large Model RLHF: PPO Principles and Code Interpretation](/07.reinforcement_learning/large_model_rlhf_ppo_principles_and_code_interpretation/large_model_rlhf_ppo_principles_and_code_interpretation.md "Large Model RLHF: PPO Principles and Code Interpretation")
    * [DPO](/07.reinforcement_learning/DPO/DPO.md "DPO")
  * [7.3 Questions](/07.reinforcement_learning/)
    * [1.rlhf related](/07.reinforcement_learning/1.rlhf_related/1.rlhf_related.md "1.rlhf related")
    * [2.Reinforcement Learning](/07.reinforcement_learning/2.reinforcement_learning/2.reinforcement_learning.md "2.Reinforcement Learning")
* [08.Retrieval Augmented Generation](/08.retrieval_augmented_generation/)
  * [8.1 RAG](/08.retrieval_augmented_generation/)
    * [Retrieval Augmented LLM](/08.retrieval_augmented_generation/retrieval_augmented_llm/retrieval_augmented_llm.md "Retrieval Augmented LLM")
    * [rag (retrieval augmented generation) technology](/08.retrieval_augmented_generation/rag_retrieval_augmented_generation_technology/rag_retrieval_augmented_generation_technology.md "rag (retrieval augmented generation) technology")
  * [8.2 Agent](/08.retrieval_augmented_generation/)
    * [Large Model Agent Technology](/08.retrieval_augmented_generation/large_model_agent_technology/large_model_agent_technology.md "Large Model Agent Technology")
* [09.LLM Evaluation](/09.llm_evaluation/)
  * [9.1 Model Evaluation](/09.llm_evaluation/)
    * [1.Evaluation](/09.llm_evaluation/1.evaluation/1.evaluation.md "1.Evaluation")
  * [9.2 LLM Hallucination](/09.llm_evaluation/)
    * [1.Large Model Hallucination](/09.llm_evaluation/1.large_model_hallucination/1.large_model_hallucination.md "1.Large Model Hallucination")
    * [2.Hallucination Sources and Mitigation](/09.llm_evaluation/2.hallucination_sources_and_mitigation/2.hallucination_sources_and_mitigation.md "2.Hallucination Sources and Mitigation")
* [10.LLM Applications](/10.llm_applications/)
  * [10.1 Chain-of-Thought Prompting](/10.llm_applications/)
    * [1.Chain-of-Thought (CoT)](/10.llm_applications/1.chain_of_thought_cot/1.chain_of_thought_cot.md "1.Chain-of-Thought (CoT)")
  * [10.2 LangChain Framework](/10.llm_applications/)
    * [1.langchain](/10.llm_applications/1.langchain/1.langchain.md "1.langchain")
* [98.Related Courses](/98.related_courses/)
* [99.References](/99.references/)







